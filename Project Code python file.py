# -*- coding: utf-8 -*-
"""ML_Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W6FbxokwDeqy0N6vzYRStsjKjmFMWKuI
"""

!pip install TPOT
import tensorflow
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from hyperopt import hp,fmin,tpe,STATUS_OK,Trials
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
import numpy as np
from tpot import TPOTClassifier
import time

"""# **Loading the dataset**"""

# Loading the iris flower dataset
iris = datasets.load_iris()
x = iris.data 
y = iris.target

# Loading the Fashion MNIST data
# (fx_train, fy_train), (fx_test,fy_test) = tensorflow.keras.datasets.fashion_mnist.load_data()
# fx_train = fx_train.reshape(fx_train.shape[0],fx_train.shape[1] * fx_train.shape[2])
# fx_test = fx_test.reshape(fx_test.shape[0],fx_test.shape[1] * fx_test.shape[2])

"""# **Splitting and preprocessing the data**"""

from sklearn import preprocessing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25 , random_state = 21)
x_train = preprocessing.normalize(x_train)
x_test = preprocessing.normalize(x_test)
x_train = x_train*2 - 1
x_test = x_test*2 - 1

"""# **Classification Problem with Random Forest Classifier by selecting hyper-parameters ourself**"""

rfc = RandomForestClassifier(criterion = "gini", max_depth = 50, max_features = 'log2', 
                               min_samples_leaf = 0.25, min_samples_split = 0.5, n_estimators = 50,
                             )
rfc.fit(x_train, y_train)

pred = rfc.predict(x_test)

print("Accuracy = ", accuracy_score(y_test, pred))

rfc.get_params

"""# **Bayesian optimization for hyperparameter search in Random Forest Classifier**"""

def Optimization(search_space):

  estimators = search_space['n_estimators']
  criterion = search_space['criterion']
  depth = search_space['max_depth']
  features = search_space['max_features']
  leaf = search_space['min_samples_leaf']
  split = search_space['min_samples_split']

  rfc_model = RandomForestClassifier(criterion = criterion, max_depth = depth, max_features = features, 
                               min_samples_leaf = leaf, min_samples_split = split, n_estimators = estimators,
                              )
    
  accuracy = cross_val_score(rfc_model, x_train, y_train, cv = 5).mean()
  return {'loss': -accuracy, 'status': STATUS_OK }

"""Defining the search space for bayesian optimization"""

c = ['entropy', 'gini']
f = ['auto', 'sqrt','log2', None]
d = np.arange(start = 10 , stop = 1001 , step = 10)
n = np.arange(start = 50 , stop = 1501 , step = 50)


search_space = {'criterion': hp.choice('criterion', c ),
        'max_depth': hp.choice('max_depth', d),
        'max_features': hp.choice('max_features', f),
        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),
        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),
        'n_estimators' : hp.choice('n_estimators', n)
    }

params = fmin(fn= Optimization,  space= search_space, algo= tpe.suggest, max_evals = 100, trials= Trials())

params

rfc = RandomForestClassifier( n_estimators = n[params['n_estimators']] ,criterion = c[params['criterion']] ,
                             max_depth = d[params['max_depth']] , max_features = f[params['max_features']] ,
                             min_samples_leaf = params['min_samples_leaf'] , min_samples_split = params['min_samples_split']
                             )

rfc.fit(x_train , y_train)

pred = rfc.predict(x_test)
print("Accuracy = ", accuracy_score(y_test, pred))

"""# **Genetic Algorithm for Hyperparameter search in Random Forest Classifier**

Defining the search space
"""

c = ['entropy', 'gini']
f = ['auto', 'sqrt','log2', None]
d = np.arange(start = 10 , stop = 1000 , step = 10)
n = np.arange(start = 50 , stop = 1501 , step = 50)
split = np.arange(0, 1, 0.001)
leaf = np.arange(0, 0.5, 0.001)

search_space = {'n_estimators': n,'criterion': c, 'max_features': f, 'max_depth': d,
                'min_samples_split': split, 'min_samples_leaf': leaf
                }

rfc_tpot = TPOTClassifier(generations= 10, population_size= 24, offspring_size= 12,
                                 verbosity= 2, early_stop= 12,
                                 config_dict={'sklearn.ensemble.RandomForestClassifier': search_space}, 
                                 cv = 5, scoring = 'accuracy')

rfc_tpot.fit(x_train , y_train)

accuracy = rfc_tpot.score(x_test, y_test)
print(accuracy)

"""# **Classification Problem with deep neural network architecture by selecting hyper-parameters ourself**

Defining the search space
"""

!pip install hyperas

!pip install np_utils

import hyperas
from hyperas import optim
from hyperas.distributions import choice, uniform
from keras.utils.np_utils import to_categorical
import keras
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation

classes = 3
y_train = to_categorical(y_train, classes)
y_test = to_categorical(y_test, classes)

"""# **Defining the model**"""

initial_model = Sequential()

initial_model.add(Dense(512, input_shape=(4,)))
initial_model.add(Activation("relu"))
initial_model.add(Dropout(0.2))

initial_model.add(Dense(256))
initial_model.add(Activation("relu"))
initial_model.add(Dropout(0.2))

initial_model.add(Dense(classes))
initial_model.add(Activation('softmax'))

initial_model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer="adam")
initial_model.fit(x_train, y_train,
              batch_size=64,
              epochs=20,              
              validation_split = 0.2)

score, acc = initial_model.evaluate(x_test, y_test, verbose=1)

"""# **Bayesian Optimization for Hyperparameter search on deep neural network architecture**

Defining the search space
"""

hlayers = [ 2, 3 , 4 , 5]
hidden_units = [ 64, 128, 256, 512 , 1024 ]
activation = [ 'relu' , 'sigmoid']
optimizer = ['adam', 'rmsprop', 'sgd']
batch_size = [ 64, 128, 256 , 512]
lr = [ 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]
epochs = np.arange(start = 10, stop = 101, step = 10)

space = {'Dense1': hp.choice('Dense1', hidden_units ),
         'Dense2': hp.choice('Dense2', hidden_units ),
         'Dense3': hp.choice('Dense3', hidden_units ),
         'Dense4': hp.choice('Dense4', hidden_units ),
         'Dense5': hp.choice('Dense5', hidden_units ),
         'Dense6': hp.choice('Dense6', hidden_units ),
         'Activation1': hp.choice('Activation1', activation ),
         'Activation2': hp.choice('Activation2', activation ),
        #  'Activation3': hp.choice('Activation3', activation ),

         'Dropout1': hp.uniform('Dropout1', 0, 1),
         'Dropout2': hp.uniform('Dropout2', 0, 1),
         'Dropout3': hp.uniform('Dropout3', 0, 1),
         'Dropout4': hp.uniform('Dropout4', 0, 1),
         'Dropout5': hp.uniform('Dropout5', 0, 1),
         'Dropout6': hp.uniform('Dropout6', 0, 1),
         
         'hidden_layers': hp.choice('hidden_layers', hlayers),
         'Optimizer': hp.choice('Optimizer', optimizer),

         'learning_rate': hp.choice('learning_rate', lr),
         'epochs': hp.choice('epochs' , epochs),
        'batch_size': hp.choice('batch_size' , batch_size)

        
    }

def Optimization_deep(space):
  model = Sequential()

  model.add(Dense(space['Dense1'], input_shape=(4,)))
  model.add(Activation(space['Activation1']))
  model.add(Dropout(space['Dropout1']))

  model.add(Dense(space['Dense2']))
  model.add(Activation(space['Activation2']))
  model.add(Dropout(space['Dropout2']))
    
  if space['hidden_layers'] >=2:
    model.add(Dense(space['Dense3']))
    model.add(Activation(space['Activation2']))
    model.add(Dropout(space['Dropout3']))

  if space['hidden_layers'] >=3:
    model.add(Dense(space['Dense4']))
    model.add(Activation(space['Activation2']))
    model.add(Dropout(space['Dropout4']))

  if space['hidden_layers'] >=4:
    model.add(Dense(space['Dense5']))
    model.add(Activation(space['Activation2']))
    model.add(Dropout(space['Dropout5']))

  if space['hidden_layers'] == 5:
    model.add(Dense(space['Dense6']))
    model.add(Activation(space['Activation2']))
    model.add(Dropout(space['Dropout6']))
        
  model.add(Dense(classes))
  model.add(Activation('softmax'))

  adam = keras.optimizers.Adam(lr=space['learning_rate'])
  rmsprop = keras.optimizers.RMSprop(lr=space['learning_rate'])
  sgd = keras.optimizers.SGD(lr=space['learning_rate'])

  temp = space['Optimizer']
  if temp == 'adam':
      optim = adam
  elif temp == 'rmsprop':
      optim = rmsprop
  else:
      optim = sgd

  model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)
  model.fit(x_train, y_train,
              batch_size = space['batch_size'],
              epochs=space['epochs'],              
              validation_split = 0.2)
  score, acc = model.evaluate(x_test, y_test, verbose=1)
  print('Test accuracy:', acc)
  return {'loss': -acc, 'status': STATUS_OK, 'model': model}

params = fmin(fn= Optimization_deep, space = space, algo= tpe.suggest, max_evals = 50, trials= Trials())

params

"""Testing neural network with the hyperparameters selected by Bayesian optimization"""

final_model = Sequential()

final_model.add(Dense(hidden_units[params['Dense1']], input_shape=(4,)))
final_model.add(Activation(activation[params['Activation1']]))
final_model.add(Dropout(params['Dropout1']))

final_model.add(Dense(hidden_units[params['Dense2']]))
final_model.add(Activation(activation[params['Activation2']]))
final_model.add(Dropout(params['Dropout2']))
    
if space['hidden_layers'] >=2:
  final_model.add(Dense(hidden_units[params['Dense3']]))
  final_model.add(Activation(activation[params['Activation3']]))
  final_model.add(Dropout(params['Dropout3']))

if space['hidden_layers'] >= 3:
  final_model.add(Dense(hidden_units[params['Dense4']]))
  final_model.add(Activation(activation[params['Activation3']]))
  final_model.add(Dropout(params['Dropout4']))

if space['hidden_layers'] >= 4:
  final_model.add(Dense(hidden_units[params['Dense5']]))
  final_model.add(Activation(activation[params['Activation3']]))
  final_model.add(Dropout(params['Dropout5']))

if space['hidden_layers'] == 5:
  final_model.add(Dense(hidden_units[params['Dense6']]))
  final_model.add(Activation(activation[params['Activation3']]))
  final_model.add(Dropout(params['Dropout6']))
        
final_model.add(Dense(classes))
final_model.add(Activation('softmax'))

adam = keras.optimizers.Adam(lr=lr[params['learning_rate']])
rmsprop = keras.optimizers.RMSprop(lr=lr[params['learning_rate']])
sgd = keras.optimizers.SGD(lr=lr[params['learning_rate']])

temp = optimizer[params['Optimizer']]
if temp == 'adam':
    optim = adam
elif temp == 'rmsprop':
    optim = rmsprop
else:
    optim = sgd

final_model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)
final_model.fit(x_train, y_train,
            batch_size= batch_size[params['batch_size']],
            epochs = epochs[params['epochs']],              
            validation_split = 0.2)

score, acc = final_model.evaluate(x_test, y_test, verbose=1)
print('Test accuracy:', acc)

"""# **Genetic Algorithm for hyperparameter search in deep neural network architecture**"""

hlayers = [ 2,3]
hidden_units = [ 64, 128, 256, 512 , 1024 ]
activation = [ 'relu' , 'logistic']
optimizer = ['adam', 'lbfgs', 'sgd']
batch_size = [ 32, 64, 128, 256 , 512]
lr = [ 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]

hidden_layer_sizes = []
for i in hidden_units:
  hidden_layer_sizes.append((i,))

for i in hidden_units:
  for j in hidden_units:
    hidden_layer_sizes.append((i,j))

for i in hidden_units:
  for j in hidden_units:
    for k in hidden_units:
      hidden_layer_sizes.append((i,j,k))

"""Defining the search space"""

search_space = {'hidden_layer_sizes': hidden_layer_sizes,
                'activation': activation,
                'solver': optimizer,
                'learning_rate_init': lr,
                'batch_size' : batch_size
                }

NN_tpot = TPOTClassifier(generations = 5, population_size= 12, offspring_size= 6,
                                 verbosity= 2, early_stop= 12,
                                 config_dict={'sklearn.neural_network.MLPClassifier': search_space}, 
                                 cv = 5, scoring = 'accuracy')

NN_tpot.fit(x_train, y_train)

accuracy = NN_tpot.score(x_test, y_test)
print(accuracy)